{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3aa71cb2",
   "metadata": {},
   "source": [
    "# Autoencoder In String Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ff10b6",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80d7a55",
   "metadata": {},
   "source": [
    "## Functions\n",
    "\n",
    "### Dependences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8dedeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install numpy\n",
    "!pip3 install torch\n",
    "!pip3 install sklearn\n",
    "!pip3 install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0fe1053d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72bc25c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, dimensions):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(dimensions[0], dimensions[1])\n",
    "        self.fc2 = nn.Linear(dimensions[1], dimensions[2])\n",
    "        self.fc3 = nn.Linear(dimensions[2], dimensions[3])\n",
    "        self.fc4 = nn.Linear(dimensions[3], dimensions[4])\n",
    "        self.fc5 = nn.Linear(dimensions[4], dimensions[5])\n",
    "\n",
    "        self.fc6 = nn.Linear(dimensions[5], dimensions[6])\n",
    "        self.fc7 = nn.Linear(dimensions[6], dimensions[7])\n",
    "        self.fc8 = nn.Linear(dimensions[7], dimensions[8])\n",
    "        self.fc9 = nn.Linear(dimensions[8], dimensions[9])\n",
    "        self.fc10 = nn.Linear(dimensions[9], dimensions[10])\n",
    "\n",
    "    def encode(self, Layer):\n",
    "        Layer = F.leaky_relu(self.fc1(Layer))\n",
    "        Layer = F.leaky_relu(self.fc2(Layer))\n",
    "        Layer = F.leaky_relu(self.fc3(Layer))\n",
    "        Layer = F.leaky_relu(self.fc4(Layer))\n",
    "        Layer = F.leaky_relu(self.fc5(Layer))\n",
    "        return Layer\n",
    "\n",
    "    def decode(self, Layer):\n",
    "        Layer = F.leaky_relu(self.fc6(Layer))\n",
    "        Layer = F.leaky_relu(self.fc7(Layer))\n",
    "        Layer = F.leaky_relu(self.fc8(Layer))\n",
    "        Layer = F.leaky_relu(self.fc9(Layer))\n",
    "        Layer = F.leaky_relu(self.fc10(Layer))\n",
    "        return Layer\n",
    "\n",
    "    def forward(self, Layer):\n",
    "        return self.decode(self.encode(Layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca34d10f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, file_name):\n",
    "        ohe = OneHotEncoder(dtype=np.int8)\n",
    "        file_out = pd.read_csv(file_name)\n",
    "\n",
    "        data_tensor = pd.DataFrame(ohe.fit_transform(file_out).toarray())\n",
    "        \n",
    "        x = data_tensor.iloc[:,:].values\n",
    "        y = file_out.iloc[:, -1].values\n",
    "\n",
    "        self.X_train = torch.tensor(x,dtype=torch.float)\n",
    "        self.y_train = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X_train[idx], self.y_train[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7acc505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset1(Dataset):\n",
    "    def __init__(self, file_name, other_file):\n",
    "        ohe = OneHotEncoder(dtype=np.int8)\n",
    "        file_out = pd.read_csv(file_name)\n",
    "        other = pd.read_csv(other_file)\n",
    "        trans = ohe.fit(file_out)\n",
    "        data_tensor = pd.DataFrame(trans.transform(other).toarray())\n",
    "        \n",
    "        x = data_tensor.iloc[:,:].values\n",
    "        y = file_out.iloc[:, -1].values\n",
    "\n",
    "        self.X_train = torch.tensor(x,dtype=torch.float)\n",
    "        self.y_train = torch.tensor(y)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_train)\n",
    "\n",
    "    def __getitem__(self, idx):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "902c5a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenght_ohe(PATH):\n",
    "    datos = pd.read_csv(PATH)\n",
    "    lengths = []\n",
    "    for column in datos.columns:\n",
    "        lengths.append(datos[column].nunique())\n",
    "    return sum(lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d46345e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lenghts_features(name_dataset):\n",
    "    datos = pd.read_csv(name_dataset)\n",
    "    lengths = []\n",
    "    for column in datos.columns:\n",
    "        lengths.append(datos[column].nunique())\n",
    "    return lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eda89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CustomLossFunction(data,output,criterion,lenghts_data):\n",
    "    ini = 0\n",
    "    train_loss = 0\n",
    "\n",
    "    for ind in lenghts_data:\n",
    "        curr_data = data[:][:,ini:ini+ind]\n",
    "        curr_target = output[:][:,ini:ini+ind]\n",
    "        train_loss += criterion(curr_target,curr_data)\n",
    "        ini = ind + ini\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba35f6e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dataloader, val_dataloader, criterion, optimizer, scheduler):\n",
    "\n",
    "    print(\"Train to:\", epocas, \"epochs\", end='\\n')\n",
    "    loggertrain = open(\"Loss_accu\"+funcion+\".txt\", \"w\")\n",
    "    start = time.time()\n",
    "    print('Start training',end='\\n')\n",
    "    \n",
    "    best_model_wts = model.state_dict()\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 0\n",
    "    best_optimizer = optimizer.state_dict()\n",
    "\n",
    "    for epoch in range(1,epocas+1):\n",
    "\n",
    "        print('Epoch {}/{}'.format(epoch, epocas))\n",
    "        print('-' * 10)\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train','valid']:\n",
    "            if phase == 'train':\n",
    "                if epoch != 1:\n",
    "                   scheduler.step()\n",
    "                model.train(True)\n",
    "                dataloader = train_dataloader\n",
    "            else:\n",
    "                model.train(False)\n",
    "                dataloader = val_dataloader\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            for (x, y) in dataloader:   \n",
    "                data = x.to(device)\n",
    "              \n",
    "                output = model(data)\n",
    "                train_loss = CustomLossFunction(data,output,criterion,lenghts_data)\n",
    "                optimizer.zero_grad()\n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    train_loss.backward()\n",
    "                    optimizer.step()\n",
    "                \n",
    "                ini = 0\n",
    "                for ind in lenghts_data:\n",
    "                    curr_target = output[:,ini:ini + ind]\n",
    "                    curr_data = data[:,ini:ini + ind]\n",
    "                    max_ind_tar = torch.max(curr_target,1)[1]\n",
    "                    max_ind_dat = torch.max(curr_data,1)[1]\n",
    "                    running_corrects += torch.sum(max_ind_tar == max_ind_dat)\n",
    "                    ini = ind + ini \n",
    "                    \n",
    "                # statistics \n",
    "                running_loss += train_loss.item()\n",
    "                        \n",
    "            epoch_loss = running_loss / len(dataloader.dataset)\n",
    "            epoch_acc = running_corrects / len(dataloader.dataset)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "            if phase == \"train\":\n",
    "                print('{}, {:.4f}, {:.4f}'.format(epoch,epoch_loss,epoch_acc),end=', ',file=loggertrain)\n",
    "            if phase == \"valid\":\n",
    "                print('{:.4f}, {:.4f}'.format(epoch_loss,epoch_acc), file=loggertrain)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'valid' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = model.state_dict()\n",
    "                best_optimizer = optimizer.state_dict()\n",
    "                best_epoch = epoch\n",
    "\n",
    "    stop = time.time()\n",
    "    hr, minu, seg = io.timetotal(start,stop)\n",
    "    print('Training finished. Time elapsed:', '{:.0f}h {:.0f}m {:.0f}s'.format(hr,minu,seg))\n",
    "\n",
    "    # Save the best model\n",
    "    save(funcion, best_model_wts, best_optimizer, best_epoch)\n",
    "   \n",
    "    loggertrain.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b990f04e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(funcion, best_model, best_optimizer, best_epoch):\n",
    "    path = \"./savedModels/model-\"+funcion+\".pt\"\n",
    "    torch.save({'model_dict': best_model,\n",
    "                'optim_dict': best_optimizer,\n",
    "                'epoch': best_epoch}, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d02d080",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(model, device, dataloader, path):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    processed = []\n",
    "    count = 0\n",
    "\n",
    "    for (x,y) in dataloader:\n",
    "        data = x.to(device)\n",
    "        processed.append(model.encode(data).cpu().detach().numpy())\n",
    "        count += len(data)\n",
    "\n",
    "    out = np.empty([count, latent])\n",
    "    index = 0\n",
    "    for batch in processed:\n",
    "        out[index:index + len(batch)] = batch\n",
    "        index += len(batch)\n",
    "\n",
    "    np.savetxt(path, out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49a0410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def success(model, device, dataloader):\n",
    "    model.eval()\n",
    "    loggerreconstruction = open(\"Success\"+funcion+\".txt\", \"w\")\n",
    "\n",
    "    for (x,y) in dataloader:\n",
    "        data = x.to(device)\n",
    "        output = model(data)\n",
    "        evalIn = np.copy(data.cpu().numpy())\n",
    "        evalOut = np.copy(output.detach().cpu().numpy())\n",
    "\n",
    "        for j in range(0, len(evalIn)):\n",
    "            correct_counter = 0\n",
    "            partial_counter = np.zeros(len(lenghts_data),dtype=np.int8)\n",
    "            ini = 0\n",
    "            k = 0\n",
    "            for ind in lenghts_data:\n",
    "                curr_target = evalIn[j][ini:ini + ind]\n",
    "                curr_data = evalOut[j][ini:ini + ind]\n",
    "                target_index = curr_target.tolist().index(np.max(curr_target))\n",
    "                data_index = curr_data.tolist().index(np.max(curr_data))\n",
    "                if target_index == data_index:\n",
    "                    partial_counter[k] = partial_counter[k] + 1\n",
    "                    correct_counter += 1\n",
    "                ini = ind + ini\n",
    "                k += 1\n",
    "            print('{},{}'.format(correct_counter,\",\".join(map(str, partial_counter.tolist()))), file=loggerreconstruction)\n",
    "\n",
    "    loggerreconstruction.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401ef8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(device, test_dataloader,dataloader_Z8,dataloader_Z12):\n",
    "\n",
    "    print('Reconstruction running', end='\\n')\n",
    "    nombre = \"./savedModels/model-\"+funcion+\".pt\"\n",
    "    dicc_model = torch.load(nombre,map_location=torch.device('cpu'))\n",
    "    model = Net(dimensions).to(device)\n",
    "    model.load_state_dict(dicc_model['model_dict'])\n",
    "    print(dicc_model['epoch'])\n",
    "\n",
    "    x1 = \"latentZ8\"+funcion\n",
    "    x2 = \"latentZ12\"+funcion\n",
    "    process(model, device, dataloader_Z8, x1)\n",
    "    process(model, device, dataloader_Z12, x2)\n",
    "    success(model, device, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction_other_models(device, dataloader_other_models):\n",
    "\n",
    "    print('Reconstruction running other models', end='\\n')\n",
    "    nombre = \"./savedModels/model-\"+funcion+\".pt\"\n",
    "    dicc_model = torch.load(nombre,map_location=torch.device('cpu'))\n",
    "    model = Net(dimensions).to(device)\n",
    "    model.load_state_dict(dicc_model['model_dict'])\n",
    "    \n",
    "    x1 = \"latentother\"+funcion\n",
    "    process(model, device, dataloader_other_models, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0547c0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def proof(device,test_dataloader):\n",
    "    \n",
    "    nombre = \"./savedModels/model-\"+funcion+\".pt\"\n",
    "    dicc_model = torch.load(nombre)\n",
    "    model = Net(dimensions).to(device)\n",
    "    model.load_state_dict(dicc_model['model_dict'])\n",
    "    \n",
    "\n",
    "    (x,y) = next(iter(test_dataloader))\n",
    "\n",
    "    data =x.to(device)\n",
    "    output = model(data)\n",
    "    evalIn = np.copy(data.cpu().numpy())\n",
    "    evalOut = np.copy(output.detach().cpu().numpy())\n",
    "\n",
    "    for j in range(0, 1):\n",
    "        ini = 0\n",
    "        k = 0\n",
    "        for ind in lenghts_data:\n",
    "            curr_target = evalIn[j][ini:ini + ind]\n",
    "            curr_data = evalOut[j][ini:ini + ind]\n",
    "            print(curr_target)\n",
    "            print(curr_data)\n",
    "\n",
    "            target_index = curr_target.tolist().index(np.max(curr_target))\n",
    "            data_index = curr_data.tolist().index(np.max(curr_data))\n",
    "            if target_index == data_index:\n",
    "                print(\"Correct\")\n",
    "            input(\"Pause\")\n",
    "            ini = ind+ini\n",
    "            k += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c7057",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(phase=\"All\"):\n",
    "\n",
    "    # Load and ohe dataset \n",
    "    print('Preparing data',end='\\n')\n",
    "    dataset = CustomDataset(datasetname)\n",
    "    other_models = CustomDataset1(datasetname,datasetname_othermodels)\n",
    "\n",
    "    # Split dataset for category\n",
    "    train_length=int(train_set*len(dataset))\n",
    "    val_length=int(val_set*len(dataset))\n",
    "    test_length=len(dataset)-train_length-val_length\n",
    "    train_dataset,val_dataset,test_dataset=torch.utils.data.random_split(dataset,(train_length,val_length,test_length))\n",
    "    \n",
    "\n",
    "    # Split dataset for geometry \n",
    "    Z8_length=292053\n",
    "    Z12_length=len(dataset)-Z8_length\n",
    "    Z8_X = dataset.X_train[:Z8_length]\n",
    "    Z8_y = dataset.y_train[:Z8_length]\n",
    "    Z12_X = dataset.X_train[Z8_length:len(dataset)]\n",
    "    Z12_y = dataset.y_train[Z8_length:len(dataset)]\n",
    "    Z8_dataset = torch.utils.data.TensorDataset(Z8_X, Z8_y)\n",
    "    Z12_dataset = torch.utils.data.TensorDataset(Z12_X, Z12_y)\n",
    "    \n",
    " \n",
    "\n",
    "    #Z8_dataset,Z12_dataset=torch.utils.data.random_split(dataset1,(Z8_length,Z12_length))\n",
    "    \n",
    "    # Dataloder train phase\n",
    "    dataloader_train=torch.utils.data.DataLoader(train_dataset,\n",
    "            batch_size=batchsize, shuffle=True, num_workers=4)\n",
    "    dataloader_val=torch.utils.data.DataLoader(val_dataset,\n",
    "            batch_size=batchsize, shuffle=True, num_workers=4)\n",
    "\n",
    "    # Dataloder reconstruction phase\n",
    "    dataloader_test=torch.utils.data.DataLoader(dataset,\n",
    "        batch_size=batchsize, shuffle=False, num_workers=4)\n",
    "    dataloader_Z8=torch.utils.data.DataLoader(Z8_dataset,\n",
    "        batch_size=batchsize, shuffle=True)\n",
    "    dataloader_Z12=torch.utils.data.DataLoader(Z12_dataset,\n",
    "        batch_size=batchsize, shuffle=True)\n",
    "\n",
    "    # Dataloader other models\n",
    "\n",
    "    dataloader_other_models = torch.utils.data.DataLoader(other_models,\n",
    "        batch_size=batchsize, shuffle=False)\n",
    "\n",
    "\n",
    "    print('Data ready',end='\\n')\n",
    "\n",
    "    # Load data in GPU\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    torch.manual_seed(1)\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    model = Net(dimensions).to(device)\n",
    "    if use_cuda:\n",
    "        print('Using GPU')\n",
    "\n",
    "    # Loss function vainilla and Optimizer     \n",
    "    criterion = nn.CrossEntropyLoss() # No es necesario\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "    exp_scheduler = lr_scheduler.StepLR(optimizer, step_size= 4000, gamma=0.1)\n",
    "\n",
    "    if phase == \"All\":\n",
    "        # Training \n",
    "        train(model, device, dataloader_train, dataloader_val, criterion, optimizer, exp_scheduler)\n",
    "        # Reconstruction\n",
    "        reconstruction(device, dataloader_test,dataloader_Z8,dataloader_Z12)\n",
    "        # Live proof\n",
    "        proof(device,dataloader_test)\n",
    "    if phase == \"T-R\":\n",
    "        # Training \n",
    "        train(model, device, dataloader_train, dataloader_val, criterion, optimizer, exp_scheduler)\n",
    "        # Reconstruction\n",
    "        reconstruction(device, dataloader_test,dataloader_Z8,dataloader_Z12)\n",
    "    if phase == \"T\":\n",
    "        # Training \n",
    "        train(model, device, dataloader_train, dataloader_val, criterion, optimizer, exp_scheduler)\n",
    "    if phase == \"R\":\n",
    "        # Reconstruction\n",
    "        reconstruction(device, dataloader_test,dataloader_Z8,dataloader_Z12)\n",
    "    if phase == \"O\":\n",
    "        reconstruction_other_models(device, dataloader_other_models)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff75441",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ---- Setup ------- #\n",
    "\n",
    "# Name input dataset\n",
    "#datasetname = './Data/Z8_Z12.csv'\n",
    "datasetname = './Data/600K2_Z12-Z8.csv'\n",
    "\n",
    "\n",
    "# Name other dataset\n",
    "datasetname_othermodels = './Data/Z12_SU5_100models.csv'\n",
    " \n",
    "# Number of epochs for train\n",
    "epocas = 1010\n",
    "\n",
    "# Parameters\n",
    "train_set = 0.6\n",
    "val_set = 0.3\n",
    "batchsize = 32\n",
    "workers = 8\n",
    "\n",
    "# Label of final files\n",
    "#funcion = str(epocas)+\"e-7leaky_rely-ADAM-CrossEntropyLoss-vainilla\"\n",
    "funcion = \"1010e-7leaky_rely-ADAM-CrossEntropyLoss-vainilla445\"\n",
    "#funcion = \"1010e-7leaky_rely-ADAM-CrossEntropyLoss-vainilla_new_dataset_seed440\"\n",
    "# latent space dimension\n",
    "latent = 3\n",
    "\n",
    "# OHE lenght\n",
    "l_ohe = io.lenght_ohe(datasetname)\n",
    "lenghts_data = io.lenghts_features(datasetname)\n",
    "\n",
    "# Dimensions of layers \n",
    "dimensions = [l_ohe, 2*l_ohe, 200, 26, 13, latent, 13, 26, 200, 2*l_ohe, l_ohe]\n",
    "\n",
    "# ----- Proccess ----- # \n",
    "\n",
    "main(\"R\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
